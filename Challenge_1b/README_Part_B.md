# Adobe India Hackathon 2025 â€“ Round 1B
## ğŸ§  Persona-Driven Document Intelligence

---

## ğŸ“Œ Overview

This project demonstrates how to extract and analyze contextually important sections from a collection of documents, tailored to a specific user **persona** and **job to be done**.

Using embeddings and similarity search (via `sentence-transformers`), the system ranks and refines relevant PDF content, producing a structured JSON output per collection.

---

## ğŸ” Problem Statement

> **Given:** A collection of PDFs  
> **Goal:** Identify the most relevant content based on the **persona** and their **intent** (job to be done).

Each collection has:
- A different **persona** (e.g., Travel Blogger, Product Trainer, Home Cook)
- A unique **job to be done** (e.g., extract travel tips, summarize features, extract recipes)

---

## ğŸ§° Tech Stack

| Component               | Tool/Library                    |
|------------------------|---------------------------------|
| Embeddings             | SentenceTransformers (MiniLM)   |
| PDF Parsing            | PyMuPDF (`fitz`)                |
| Ranking                | Cosine Similarity (SBERT)       |
| Output Format          | JSON                            |
| Language               | Python 3                        |

---

## ğŸ—‚ï¸ Project Structure

```
.
â”œâ”€â”€ Collection 1/
â”‚   â””â”€â”€ PDFs/
â”œâ”€â”€ Collection 2/
â”‚   â””â”€â”€ PDFs/
â”œâ”€â”€ Collection 3/
â”‚   â””â”€â”€ PDFs/
â”œâ”€â”€ output/
â”‚   â””â”€â”€ collection_1_output.json
â”‚   â””â”€â”€ collection_2_output.json
â”‚   â””â”€â”€ collection_3_output.json
â”œâ”€â”€ analyze_all_collections.py
â””â”€â”€ README.md
```

---

## ğŸ§ª How to Run

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

**`requirements.txt`** (for your convenience):

```txt
sentence-transformers
pymupdf
```

---

### 2. Add Your PDF Files

Place your PDFs inside the respective folders:
- `Collection 1/PDFs`
- `Collection 2/PDFs`
- `Collection 3/PDFs`

---

### 3. Run the Script

```bash
python analyze_all_collections.py
```

This will:
- Read all PDFs
- Tokenize and encode content
- Rank sections based on persona-job relevance
- Output structured results to `output/` as JSON

---

## ğŸ“¤ Output Format

Each output JSON includes:

- `metadata`: input files, persona, job, timestamp
- `extracted_sections`: top 5 important sections (title, score, page)
- `sub_section_analysis`: actual refined content of the above sections

Sample:
```json
{
  "metadata": {
    "persona": "Travel Blogger",
    "job_to_be_done": "Summarize the best travel tips...",
    "input_documents": ["doc1.pdf", "doc2.pdf"],
    "timestamp": "2025-07-28T12:45:00Z"
  },
  "extracted_sections": [
    {
      "document": "doc1.pdf",
      "section_title": "Top 10 tips for the French Riviera...",
      "importance_score": 0.87,
      "page_number": 4
    }
  ],
  "sub_section_analysis": [
    {
      "document": "doc1.pdf",
      "refined_text": "If you're visiting Nice in summer...",
      "page_number": 4
    }
  ]
}
```

---

## ğŸ§  Customization

You can easily add more collections or tweak the persona/job definitions inside the `COLLECTIONS` dictionary in `analyze_all_collections.py`.

---

## âœ… Status

- âœ… Completed full pipeline for multi-document and multi-persona document intelligence
- âœ… Outputs context-rich summaries aligned with user personas

---

## ğŸ Team

- **Participant:** [Your Name]
- **Hackathon:** Adobe India Hackathon 2025 â€“ Round 1B